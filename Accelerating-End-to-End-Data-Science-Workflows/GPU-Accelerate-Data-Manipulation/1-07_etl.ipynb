{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b754f1a2-24e8-44d4-ae6a-257483573434",
   "metadata": {},
   "source": [
    "# Accelerating End-to-End Data Science Workflows # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881e48fa-78ac-4d92-a8bb-d419c14df9e9",
   "metadata": {},
   "source": [
    "## 07 - การแยก, แปลง, และโหลด (Extract, Transform, and Load) ##\n",
    "\n",
    "**สารบัญ**\n",
    "\n",
    "ในโน้ตบุ๊กนี้ เราจะเรียนรู้พื้นฐานของการแยก, แปลง, และโหลด โน้ตบุ๊กนี้ครอบคลุมส่วนต่างๆ ดังนี้:\n",
    "\n",
    "1.  [การแยก, แปลง, และโหลด (ETL)](#Extract,-Transform,-and-Load-(ETL))\n",
    "    * [การแยก (Extract)](#Extract)\n",
    "    * [การแปลง (Transform)](#Transform)\n",
    "    * [การโหลด (Load)](#Load)\n",
    "2.  [บันทึกเป็นรูปแบบ Parquet (Save to Parquet Format)](#Save-to-Parquet-Format)\n",
    "    * [การอ่านจาก Parquet (Reading from Parquet)](#Reading-from-Parquet)\n",
    "3.  [การเร่งความเร็ว ETL สำหรับงานปลายน้ำ (Accelerated ETL for Downstream Tasks)](#Accelerated-ETL-for-Downstream-Tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a083cd6e-73f4-42b8-be04-bdd2bdeb5185",
   "metadata": {},
   "source": [
    "## การแยก, แปลง, และโหลด (ETL) ##\n",
    "\n",
    "กรณีการใช้งานที่สำคัญ แต่อาจไม่ได้รับการยกย่องเท่าที่ควรของ RAPIDS คือการแยก, แปลง, และโหลด หรือเรียกสั้นๆ ว่า ETL ซึ่งเป็นกระบวนการรวมข้อมูลที่ใช้เพื่อรวมข้อมูลจากหลายแหล่งเข้าเป็นที่เก็บข้อมูลเดียวที่สอดคล้องกัน เป้าหมายหลักคือ:\n",
    "\n",
    "* รวมข้อมูลจากหลายแหล่งให้เป็นรูปแบบเดียวที่สอดคล้องกัน\n",
    "* ปรับปรุงคุณภาพข้อมูลผ่านการทำความสะอาดและการตรวจสอบ\n",
    "* ช่วยให้การวิเคราะห์ข้อมูลและการรายงานมีประสิทธิภาพมากขึ้น\n",
    "* สนับสนุนการตัดสินใจที่ขับเคลื่อนด้วยข้อมูล"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d5d73e-1665-4706-8840-6e14c2fabaa7",
   "metadata": {},
   "source": [
    "### การแยก (Extract) ###\n",
    "\n",
    "**การแยก** คือขั้นตอนแรกที่ข้อมูลจะถูกรวบรวมจากระบบแหล่งที่มาต่างๆ แหล่งที่มาเหล่านี้อาจรวมถึง:\n",
    "* ไฟล์คงที่ (csv, json)\n",
    "* SQL RDBMS\n",
    "* เว็บเพจ\n",
    "* API\n",
    "\n",
    "**หมายเหตุ**: cuDF ไม่มีวิธีดึงข้อมูลธุรกรรมจากฐานข้อมูล SQL ภายนอกไปยัง GPU โดยตรง วิธีแก้ปัญหาคือการอ่านด้วย pandas และสร้าง cuDF dataframe ด้วย `cudf.from_pandas()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6534488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17be68c6-49c0-429d-86d2-c2cb711a6dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cudf.pandas\n",
    "# DO NOT CHANGE THIS CELL\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c75532-1bb6-4aab-9cee-3215d7137cee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "dtype_dict={\n",
    "    'age': 'int8', \n",
    "    'sex': 'object', \n",
    "    'county': 'object', \n",
    "    'lat': 'float32', \n",
    "    'long': 'float32', \n",
    "    'name': 'object'\n",
    "}\n",
    "        \n",
    "df=pd.read_csv('./data/uk_pop.csv', dtype=dtype_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd97bfdd-5ead-4cd7-bc17-0e1eeb4aa62b",
   "metadata": {},
   "source": [
    "เมื่อนำเข้าข้อมูล สิ่งสำคัญคือการรวมเฉพาะคอลัมน์ที่เกี่ยวข้องเพื่อลดภาระหน่วยความจำและการคำนวณ\n",
    "\n",
    "ด้านล่างนี้ เราจะอ่านข้อมูลจุดศูนย์กลางมณฑล\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d064ff45-4cf4-48cd-9cbe-3f3f177fb875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "centroid_df=pd.read_csv('county_centroid.csv')\n",
    "centroid_df.columns=['county', 'lat_county_center', 'long_county_center']\n",
    "centroid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbf6f18-f28d-45ff-a4c4-1f50bfcf5860",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cudf.pandas.line_profile\n",
    "combined_df=df.merge(centroid_df, on='county')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3294d6e0-00d9-4e33-91d8-2dc02db51512",
   "metadata": {},
   "source": [
    "\n",
    "### การแปลง (Transform) ###\n",
    "\n",
    "ในขั้นตอน **การแปลง** ข้อมูลที่ถูกดึงออกมาจะถูกทำความสะอาด ตรวจสอบความถูกต้อง และแปลงเป็นรูปแบบที่เหมาะสมสำหรับการวิเคราะห์\n",
    "\n",
    "ด้านล่างนี้ เราจะเพิ่มคอลัมน์ใหม่ ซึ่งแสดงถึงระยะทางของแต่ละบุคคลจากจุดศูนย์กลางมณฑลของตน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ce3e0-625b-49ec-a755-2bfddee41431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cudf.pandas.line_profile\n",
    "c=['lat', 'long']\n",
    "combined_df['R']=((combined_df[c] - combined_df.groupby('county')[c].transform('mean')) ** 2).sum(axis=1) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e676b1b-143d-4cf4-a534-8c25dafac1a6",
   "metadata": {},
   "source": [
    "การใช้การ join เพื่อดึงค่า lookup สามารถทำได้เร็วกว่าการคำนวณค่าเหล่านั้นขึ้นมาใหม่ ไม่ใช่เรื่องแปลกที่จะเก็บสถิติของกลุ่มไว้เพื่อจุดประสงค์นี้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4339b0c7-c391-48f4-9ba3-00f79ba50b5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cudf.pandas.line_profile\n",
    "\n",
    "# read in centroid data\n",
    "centroid_df=pd.read_csv('county_centroid.csv')\n",
    "\n",
    "# merge \n",
    "combined_df=df.merge(centroid_df, on='county', suffixes=['', '_county_center'])\n",
    "\n",
    "# calculate distance from county center\n",
    "combined_df['R']=((combined_df['lat']-combined_df['lat_county_center'])**2+(combined_df['long']-combined_df['long_county_center'])**2)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa017ea0-4ad2-4184-9669-40747e31381a",
   "metadata": {},
   "source": [
    "ด้านล่างนี้ เราจะกรองข้อมูลเพื่อรวมเฉพาะผู้ใหญ่\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917aa1ed-1778-40d4-bbbc-5892935ce7cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cudf.pandas.line_profile\n",
    "\n",
    "senior_df_filter=combined_df['age'] >= 60\n",
    "senior_df=combined_df.loc[senior_df_filter]\n",
    "\n",
    "display(senior_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb474531-389e-44aa-a6b3-8de205d9cb0b",
   "metadata": {},
   "source": [
    "### การโหลด (Load) ###\n",
    "\n",
    "ขั้นตอนสุดท้ายคือ **การโหลด** ซึ่งข้อมูลที่ผ่านการแปลงจะถูกโหลดเข้าสู่ระบบเป้าหมาย ระบบเป้าหมายอาจเป็นฐานข้อมูลหรือไฟล์ สิ่งสำคัญคือต้องเป็นระบบที่มีประสิทธิภาพสำหรับงานปลายน้ำ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f637229-1770-439b-9702-6356723c96f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "senior_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348eac86-3d67-42e2-9c7f-e0acb14021cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "senior_df.to_csv('senior_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25ae20b-7eea-4775-9743-d43cbf6877a7",
   "metadata": {},
   "source": [
    "**หมายเหตุ**: หากงานปลายน้ำเกี่ยวข้องกับการสอบถามและวิเคราะห์ข้อมูลเพิ่มเติม รูปแบบไฟล์ CSV อาจไม่ใช่ตัวเลือกที่ดีที่สุด"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08413512-47f9-4543-a80c-442f1500b49a",
   "metadata": {},
   "source": [
    "<a name='s1-6'></a>\n",
    "## บันทึกในรูปแบบ Parquet ##\n",
    "\n",
    "หลังจากประมวลผลข้อมูลแล้ว เราจะคงข้อมูลนั้นไว้เพื่อใช้ในภายหลัง [Apache Parquet](https://parquet.apache.org/) เป็นรูปแบบไบนารีแบบคอลัมน์ และได้กลายเป็นมาตรฐานโดยพฤตินัยสำหรับการจัดเก็บข้อมูลตารางปริมาณมาก การแปลงเป็นรูปแบบไฟล์ Parquet มีความสำคัญ และโดยทั่วไปควรหลีกเลี่ยงไฟล์ CSV ในผลิตภัณฑ์ข้อมูล แม้ว่ารูปแบบไฟล์ CSV จะสะดวกและอ่านได้ง่าย แต่การนำเข้าไฟล์ CSV จำเป็นต้องอ่านและแยกวิเคราะห์เรคคอร์ดทั้งหมด ซึ่งอาจเป็นคอขวด ในความเป็นจริง นักพัฒนาจำนวนมากจะเริ่มการวิเคราะห์โดยการแปลงไฟล์ CSV เป็นรูปแบบไฟล์ Parquet ก่อน มีเหตุผลมากมายที่ต้องใช้รูปแบบ Parquet สำหรับการวิเคราะห์:\n",
    "\n",
    "* ลักษณะแบบคอลัมน์ของไฟล์ Parquet ช่วยให้สามารถทำการตัดคอลัมน์ได้ ซึ่งมักจะทำให้ประสิทธิภาพการสืบค้นข้อมูลดีขึ้นอย่างมาก\n",
    "* ใช้เมตาดาตาเพื่อจัดเก็บ Schema และรองรับประเภทข้อมูลขั้นสูงมากขึ้น เช่น Categorical, Datetimes และอื่นๆ ซึ่งหมายความว่าการนำเข้าข้อมูลจะไม่ต้องมีการอนุมาน Schema หรือการระบุ Schema ด้วยตนเอง\n",
    "* รวบรวมเมตาดาตาที่เกี่ยวข้องกับสถิติระดับ Row-group สำหรับแต่ละคอลัมน์ ซึ่งช่วยให้สามารถกรองแบบ Predicate pushdown ซึ่งเป็นการสืบค้นแบบ Pushdown ที่ช่วยให้การคำนวณเกิดขึ้นที่ \"เลเยอร์ฐานข้อมูล\" แทนที่จะเป็น \"เลเยอร์เอนจินการดำเนินการ\" ในกรณีนี้ เลเยอร์ฐานข้อมูลคือไฟล์ Parquet ในระบบไฟล์ และเอนจินการดำเนินการคือ Dask\n",
    "* รองรับตัวเลือกการบีบอัดที่ยืดหยุ่น ทำให้จัดเก็บได้กะทัดรัดและพกพาสะดวกกว่าฐานข้อมูล\n",
    "\n",
    "เราจะใช้ `.to_parquet(path)`[[doc]](https://docs.dask.org/en/stable/generated/dask.dataframe.to_parquet.html#dask-dataframe-to-parquet) เพื่อเขียนลงในไฟล์ Parquet โดยค่าเริ่มต้น ไฟล์จะถูกสร้างขึ้นในไดเร็กทอรีเอาต์พุตที่ระบุโดยใช้หลักการ `part.0.parquet`, `part.1.parquet`, `part.2.parquet`, ... และอื่นๆ สำหรับแต่ละพาร์ติชันใน DataFrame สิ่งนี้สามารถเปลี่ยนแปลงได้โดยใช้พารามิเตอร์ `name_function` การส่งออกหลายไฟล์ช่วยให้ Dask สามารถเขียนไปยังหลายไฟล์พร้อมกัน ซึ่งเร็วกว่าการเขียนไปยังไฟล์เดียว\n",
    "\n",
    "<p><img src='images/parquet.png' width=240></p>\n",
    "\n",
    "เมื่อทำงานกับชุดข้อมูลขนาดใหญ่ การถอดรหัสและการเข้ารหัสมักจะเป็นงานที่ใช้ต้นทุนสูง ความท้าทายนี้มักจะซับซ้อนขึ้นเมื่อขนาดข้อมูลเพิ่มขึ้น รูปแบบที่พบบ่อยในวิทยาศาสตร์ข้อมูลคือการแบ่งชุดข้อมูลตามคอลัมน์ การแบ่งแถว หรือทั้งสองอย่าง การย้ายการดำเนินการกรองเหล่านี้ไปยังเฟสการอ่านของเวิร์กโฟลว์สามารถ: 1) ลดเวลา I/O และ 2) ลดปริมาณหน่วยความจำที่จำเป็น ซึ่งเป็นสิ่งสำคัญสำหรับ GPU ที่หน่วยความจำอาจเป็นปัจจัยจำกัด รูปแบบไฟล์ Parquet ช่วยให้การอ่านแบบกรองเป็นไปได้ผ่าน **การตัดคอลัมน์ (column pruning)** และ **การกรอง Predicate แบบอิงสถิติ (statistic-based predicate filtering)** เพื่อข้ามส่วนของข้อมูลที่ไม่เกี่ยวข้องกับปัญหา ด้านล่างนี้คือเคล็ดลับบางประการสำหรับการเขียนไฟล์ Parquet:\n",
    "\n",
    "* เมื่อเขียนข้อมูล การเรียงลำดับข้อมูลตามคอลัมน์ที่คาดว่าจะมีการใช้ฟิลเตอร์มากที่สุด หรือคอลัมน์ที่มีคาร์ดินาลิตีสูงสุด สามารถนำไปสู่ประโยชน์ด้านประสิทธิภาพอย่างมาก เมตาดาตาที่คำนวณสำหรับแต่ละกลุ่มแถวจะช่วยให้สามารถใช้ฟิลเตอร์ Predicate pushdown ได้อย่างเต็มที่\n",
    "* การเขียนรูปแบบ Parquet ซึ่งต้องมีการประมวลผลชุดข้อมูลทั้งหมดใหม่ อาจมีต้นทุนสูง รูปแบบนี้ทำงานได้ดีอย่างน่าทึ่งสำหรับการใช้งานที่เน้นการอ่าน และการจัดเก็บและดึงข้อมูลที่มีความหน่วงต่ำ\n",
    "* พาร์ติชันใน Dask DataFrame สามารถเขียนไฟล์พร้อมกันได้ ดังนั้นไฟล์ Parquet หลายไฟล์จึงถูกเขียนพร้อมกัน\n",
    "\n",
    "ด้านล่างนี้ เราจะเขียนข้อมูลในรูปแบบ Parquet หลังจากเรียงลำดับตามมณฑล"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35007914-f7af-4083-a42a-fc02048c7ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "senior_df=senior_df.sort_values('county')\n",
    "\n",
    "senior_df.to_parquet('senior_df.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e82b557-a2c2-4a42-96d0-35174370aaee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3438a6-02d8-4ea0-824f-08c4bcac1063",
   "metadata": {},
   "source": [
    "### การอ่านจาก Parquet ###\n",
    "\n",
    "การสอบถามข้อมูลในรูปแบบ Parquet สามารถทำได้เร็วกว่ามาก โดยเฉพาะอย่างยิ่งเมื่อขนาดข้อมูลเพิ่มขึ้น\n",
    "\n",
    "ด้านล่างนี้ เราจะอ่านจากทั้งรูปแบบ CSV และ Parquet เพื่อเปรียบเทียบ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c2aab8-3f67-4a57-8eaa-b5d4b224a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cudf.pandas\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3876ba06-ef43-4849-8a16-58ee2f3a8423",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cudf.pandas.line_profile\n",
    "\n",
    "sel=[('county', '=', 'BLACKPOOL')]\n",
    "parquet_df=pd.read_parquet('senior_df.parquet', columns=['age', 'sex', 'county', 'lat', 'long', 'name', 'R'], filters=sel)\n",
    "parquet_df=parquet_df.loc[parquet_df['county']=='BLACKPOOL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c67aaf7-4dda-4bf5-9205-fd097d567ea4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parquet_df['county'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a225a7-3106-48a3-b100-24c829a6089c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cudf.pandas.line_profile\n",
    "\n",
    "df=pd.read_csv('./senior_df.csv', usecols=['age', 'sex', 'county', 'lat', 'long', 'name', 'R'])\n",
    "df=df.loc[df['county']=='BLACKPOOL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b5fff0-ccfa-47fe-b821-d68c5858f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['county'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdf4ff5-c182-41e5-b17d-3c0cd02dd9d5",
   "metadata": {},
   "source": [
    "## การเร่งความเร็ว ETL สำหรับงาน DownStream  ##\n",
    "\n",
    "การเร่งกระบวนการ ETL เป็นสิ่งสำคัญสำหรับวิทยาศาสตร์ข้อมูล เนื่องจากให้ประโยชน์ดังต่อไปนี้:\n",
    "\n",
    "* **ข้อมูลเชิงลึกที่ทันเวลา**: ETL ที่เร็วขึ้นช่วยให้การวิเคราะห์ข้อมูลเป็นปัจจุบันมากขึ้น ทำให้นักวิทยาศาสตร์ข้อมูลสามารถทำงานกับข้อมูลล่าสุดได้\n",
    "* **เพิ่มประสิทธิภาพการทำงาน**: เวลาประมวลผลที่ลดลงหมายความว่านักวิทยาศาสตร์ข้อมูลสามารถใช้เวลาในการวิเคราะห์และพัฒนาโมเดลได้มากขึ้น แทนที่จะรอให้ข้อมูลพร้อม\n",
    "* **การจัดการชุดข้อมูลขนาดใหญ่**: กระบวนการ ETL ที่เร่งความเร็วสามารถจัดการข้อมูลปริมาณมากได้อย่างมีประสิทธิภาพมากขึ้น\n",
    "* **ประสิทธิภาพด้านต้นทุน**: ETL ที่เร่งความเร็วสามารถลดทรัพยากรการคำนวณและเวลา ซึ่งนำไปสู่ต้นทุนโครงสร้างพื้นฐานที่ต่ำลง\n",
    "\n",
    "<p><img src='images/etl.png' width=720></p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5105158c-ce64-4b8c-9ee7-6d5d684ea5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88554b0-23c6-4316-912e-f962b4c97456",
   "metadata": {},
   "source": [
    "**เยี่ยมมาก!** ไปยัง [โน้ตบุ๊กถัดไป](1-08_cudf-polars.ipynb) กันเลย"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
