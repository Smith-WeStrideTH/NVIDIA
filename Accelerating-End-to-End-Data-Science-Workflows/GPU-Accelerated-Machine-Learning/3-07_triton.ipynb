{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57b540f-18e8-42c3-a688-c30103c5f462",
   "metadata": {},
   "source": [
    "# Accelerating End-to-End Data Science Workflows # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa05f5-febf-487c-bd59-d489efd98709",
   "metadata": {},
   "source": [
    "## 07 - Triton ##\n",
    "\n",
    "**สารบัญ**\n",
    "<br>\n",
    "สมุดบันทึก (notebook) นี้จะอธิบายถึงกระบวนการการปรับใช้โมเดลด้วย Triton Inference Server และพาคุณไปทำความเข้าใจเครื่องมือต่างๆ ที่มีอยู่ใน Triton โดยครอบคลุมหัวข้อด้านล่างนี้:\n",
    "1. [ความเป็นมา](#s1)\n",
    "2. [การเตรียมโมเดล](#s1)\n",
    "    * [โหลดโมเดล](#s1)\n",
    "    * [การสร้างโครงสร้างโฟลเดอร์](#s1)\n",
    "    * [การสร้างไฟล์กำหนดค่า](#s1)\n",
    "3. [การโหลดโมเดลใน Triton](#s1)\n",
    "    * [การเริ่มต้น Triton](#s1)\n",
    "    * [ตรวจสอบสถานะของ Triton Server](#s1)\n",
    "4. [การทดสอบการอนุมาน (Inference)](#s1)\n",
    "    * [Triton Client](#s1)\n",
    "    * [การตรวจสอบผลลัพธ์สำหรับโมเดลโลคัลและ Triton](#s1)\n",
    "5. [การวิเคราะห์ประสิทธิภาพ](#s1)\n",
    "    * [การปรับแต่ง Perf Analyzer](#s1)\n",
    "    * [แบบฝึกหัดที่ 1 - การทดสอบ Perf Analyzer](#s1)\n",
    "6. [Model Analyzer](#s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d08487-8161-4d8c-b402-cdeab3fcbe2a",
   "metadata": {},
   "source": [
    "# ความเป็นมา\n",
    "NVIDIA มีเฟรมเวิร์กสำหรับการนำโมเดล ML ไปใช้งานที่เรียกว่า **Triton** โดย Triton จะจัดการคำขอ Inference (การอนุมานผล) ทั้งหมดที่เข้ามายังเซิร์ฟเวอร์โดยอัตโนมัติ Triton รองรับแบ็กเอนด์หลายตัว เช่น PyTorch, TensorFlow, Forest Inference Library (FIL) เป็นต้น ในหน้านี้ (notebook) เราจะเน้นไปที่ **FIL backend** โดยใช้โมเดล xgboost ที่ได้ฝึกฝนไว้ก่อนหน้านี้"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0a7d94-c0a9-4b86-92bb-fafffa1ed021",
   "metadata": {},
   "source": [
    "## การเตรียมโมเดล\n",
    "### โหลดโมเดล\n",
    "มาเริ่มต้นด้วยการโหลดโมเดล XGBoost ตัวก่อนหน้ากันครับ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b2149c-3f83-46cd-a1eb-70db7067cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "model = xgb.Booster({'nthread': 4})  # init model\n",
    "model.load_model('xgboost_model.json')  # load model data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7896106-8410-40c3-8b6d-e0bdb94f69c3",
   "metadata": {},
   "source": [
    "### สร้างโครงสร้างโฟลเดอร์\n",
    "ใน Jupyter Notebook ก่อนหน้า เราได้บันทึกโมเดล XGBoost ไว้ใน working directory เท่านั้น แต่ Triton ต้องการให้โมเดลอยู่ในโครงสร้างเฉพาะ: **ชื่อโมเดล** ควรเป็นไดเรกทอรีระดับบนสุด และ **หมายเลขเวอร์ชัน** ควรอยู่ถัดไป ซึ่งช่วยให้สามารถโฮสต์โมเดลและเวอร์ชันต่างๆ ของโมเดลเหล่านั้นได้พร้อมกัน มาสร้างโครงสร้างโฟลเดอร์และบันทึกโมเดลกันเถอะ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c42b97-2691-46d1-9b3a-7d19685d1d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create the model repository directory. The name of this directory is arbitrary.\n",
    "REPO_PATH = os.path.abspath('models')\n",
    "os.makedirs(REPO_PATH, exist_ok=True)\n",
    "\n",
    "# The name of the model directory determines the name of the model as reported by Triton\n",
    "model_dir = os.path.join(REPO_PATH, \"virus_prediction\")\n",
    "\n",
    "# We can store multiple versions of the model in the same directory. In our case, we have just one version, so we will add a single directory, named '1'.\n",
    "version_dir = os.path.join(model_dir, '1')\n",
    "os.makedirs(version_dir, exist_ok=True)\n",
    "\n",
    "# The default filename for XGBoost models saved in json format is 'xgboost.json'.\n",
    "# It is recommended that you use this filename to avoid having to specify a name in the configuration file.\n",
    "model_file = os.path.join(version_dir, 'xgboost.json')\n",
    "model.save_model(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e5837-4807-4fd4-af21-0973a4942f1b",
   "metadata": {},
   "source": [
    "### สร้างไฟล์การตั้งค่า (Configuration File)\n",
    "Triton ยังต้องการ **ไฟล์การตั้งค่า** ที่ให้รายละเอียดเกี่ยวกับโมเดลและการนำไปใช้งาน สำหรับหน้านี้ (notebook) เราจะใช้ค่าพารามิเตอร์เริ่มต้น"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d898fb-a8d2-4d1c-a13f-2c4be6c18969",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_text = f\"\"\"backend: \"fil\"\n",
    "max_batch_size: {32768}\n",
    "input [                                 \n",
    " {{  \n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 4 ]                    \n",
    "  }} \n",
    "]\n",
    "output [\n",
    " {{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "  }}\n",
    "]\n",
    "instance_group [{{ kind: KIND_GPU }}]\n",
    "parameters [\n",
    "  {{\n",
    "    key: \"model_type\"\n",
    "    value: {{ string_value: \"xgboost_json\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"output_class\"\n",
    "    value: {{ string_value: \"false\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"storage_type\"\n",
    "    value: {{ string_value: \"AUTO\" }}\n",
    "  }}\n",
    "]\n",
    "\n",
    "dynamic_batching {{\n",
    "  max_queue_delay_microseconds: 100\n",
    "}}\"\"\"\n",
    "config_path = os.path.join(model_dir, 'config.pbtxt')\n",
    "with open(config_path, 'w') as file_:\n",
    "    file_.write(config_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f262d4d9-819a-4d28-bb26-7eb737c3387a",
   "metadata": {},
   "source": [
    "ทีนี้โมเดลก็พร้อมที่จะโหลดลงใน Triton แล้ว! โครงสร้างของโมเดล (model repository) ควรมีลักษณะดังนี้ครับ\n",
    "```\n",
    "model/\n",
    "`-- virus_prediction\n",
    "    |-- 1\n",
    "    |   `-- xgboost.model\n",
    "    `-- config.pbtxt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5876d7a-3416-4ac9-827a-53796a1f7f93",
   "metadata": {},
   "source": [
    "## การโหลดโมเดลใน Triton\n",
    "ถัดไป เราจะต้องเริ่ม **Triton server** สำหรับคอร์สเรียนนี้ เซิร์ฟเวอร์ได้ถูกเริ่มไว้แล้ว แต่เราจะกล่าวถึงขั้นตอนที่จำเป็นโดยย่อ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709cf499-9faa-4746-9252-16f41fa548e6",
   "metadata": {},
   "source": [
    "### การเริ่มต้นใช้งาน Triton\n",
    "\n",
    "Triton มีให้เลือกใช้งานทั้งในรูปแบบซอร์สโค้ดที่สามารถคอมไพล์ได้ หรืออิมเมจ Docker ที่สร้างไว้ล่วงหน้าแล้ว เพื่อความง่าย เราแนะนำให้ผู้ใช้ส่วนใหญ่เริ่มต้นใช้งานด้วยอิมเมจ Docker นี่คือวิธีการที่คุณจะสามารถรัน Docker container ในคอนโซลได้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283515c-c812-41c5-a100-e4dc5788781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!docker run --gpus=1 --rm -p 8000:8000 -p 8001:8001 -p 8002:8002 -v /full/path/to/docs/examples/model_repository:/models nvcr.io/nvidia/tritonserver:<xx.yy>-py3 tritonserver --model-repository=/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ded6e36-0a1d-488e-8b10-f482ced8ea07",
   "metadata": {},
   "source": [
    "ว้าว! มี Inputs เยอะเลยใช่ไหมครับ มาทำความเข้าใจกันทีละส่วนดีกว่า:\n",
    "\n",
    "* **gpus=1**: ส่งผ่าน GPU ตัวแรกไปยัง Triton Inference Server\n",
    "* **rm**: ลบคอนเทนเนอร์หลังจากดำเนินการเสร็จสิ้น\n",
    "* **p 8000:8000**: ส่งต่อพอร์ต GTPCInferenceService\n",
    "* **p 8001:8001**: ส่งต่อพอร์ต HTTPService\n",
    "* **p 8002:8002**: ส่งต่อพอร์ต Metrics\n",
    "* **v /full/path/to/docs/examples/model_repository:/models**: เมาท์ (mount) พาธของโฟลเดอร์โมเดลบนเครื่องโฮสต์ไปยังคอนเทนเนอร์ Triton Inference Server\n",
    "* **nvcr.io/nvidia/tritonserver:<xx.yy>-py3**: ชื่อของอิมเมจ Triton Inference Server หมายเลขเวอร์ชันจะเปลี่ยนไปตามรุ่นล่าสุดที่ออกมา\n",
    "* **tritonserver --model-repository=/models**: คำสั่งที่ใช้รันในคอนเทนเนอร์ ในกรณีนี้ เราจะเริ่ม Triton Inference Server และชี้ไปยังโฟลเดอร์ `models`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42967a82-fdfd-413e-8006-eb78a3ca805b",
   "metadata": {},
   "source": [
    "ดังที่เราได้กล่าวไปก่อนหน้านี้ **เซิร์ฟเวอร์ได้ถูกเริ่มต้น** สำหรับแล็บนี้เรียบร้อยแล้ว **มาตรวจสอบการเชื่อมต่อ** ของเรากับเซิร์ฟเวอร์กัน! เราใช้ \"triton\" เป็นชื่อโฮสต์ (hostname) เนื่องจากเครือข่าย Docker เริ่มต้นจะทำการแปลง \"triton\" ให้เป็นที่อยู่ IP ของ Triton Inference Server ครับ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac240cdc-f0dc-45ad-9838-58066a85696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92b9b35-51c6-4eba-a9c0-9a92d19e0a0b",
   "metadata": {},
   "source": [
    "ตอนนี้มาดูกันว่าโมเดลโหลดเรียบร้อยหรือยัง!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80b37c5-1242-489f-8463-387e657264a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST http://triton:8000/v2/repository/index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06902d34-7763-4693-b919-f70ee4d60dfa",
   "metadata": {},
   "source": [
    "หากทุกอย่างเป็นไปได้ด้วยดี เราก็น่าจะเห็นโมเดล **\"การทำนายไวรัส (virus prediction)\"** แสดงสถานะเป็น **พร้อมใช้งาน (ready)** ครับ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2429b1bf-9372-42a5-86ea-be07cabf30ed",
   "metadata": {},
   "source": [
    "## การทดสอบการอนุมาน (Testing Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2616f6-2de8-4e84-96d1-ad92b5f4b732",
   "metadata": {},
   "source": [
    "### Triton Client\n",
    "เพื่อทดสอบการติดตั้งใช้งาน (deployment) เราจะใช้ไลบรารี Triton Client มาดูกันว่าเราจะสร้างอินสแตนซ์ของไคลเอนต์ได้อย่างไร"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec652ce4-a2d6-4b3a-8e2f-faac51da5487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tritonclient.grpc as triton_grpc\n",
    "from tritonclient import utils as triton_utils\n",
    "HOST = \"triton\"\n",
    "PORT = 8001\n",
    "TIMEOUT = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22af3af5-1d15-406f-91cb-2208c938d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = triton_grpc.InferenceServerClient(url=f'{HOST}:{PORT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c3a594-e377-47c9-b3e7-0da321138bf5",
   "metadata": {},
   "source": [
    "ตอนนี้ เรามาตรวจสอบให้แน่ใจว่า Triton server พร้อมใช้งานแล้ว โดยการส่งคำขอ inference ตัวอย่าง ก่อนอื่นเรามาโหลดข้อมูลสำหรับ train (training data) เราจะโหลดเพียง **32768 แถว** เท่านั้น เนื่องจากเป็นขนาด batch สูงสุดที่เรากำหนดไว้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57396e8-bc06-43dd-bf4c-2e34b24f5962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf \n",
    "import numpy as np\n",
    "df = cudf.read_csv('./data/clean_uk_pop_full.csv', usecols=['age', 'sex', 'northing', 'easting', 'infected'], nrows=5000000)\n",
    "df = df.sample(32768)\n",
    "input_data = df.drop('infected', axis=1)\n",
    "target = df[['infected']]\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24f3792-a541-4b9f-acdc-e72c64897804",
   "metadata": {},
   "source": [
    "ตอนนี้เราจะแปลงให้อยู่ในรูปแบบ **Numpy array** และกำหนดชนิดข้อมูลให้เป็น **float32** (ชนิดเดียวกันกับที่เรากำหนดไว้ในไฟล์ตั้งค่า)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab43811-f273-48cf-b473-ea3d4786cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_df = input_data.to_numpy(dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed49f912-5e35-4fe8-b6fa-b91f025cc07e",
   "metadata": {},
   "source": [
    "เนื่องจากเราจำกัดขนาดแบตช์ (batch size) ไว้ที่ 32768 ดังนั้น เรามาทำการแบ่งอาร์เรย์ (splice the array) และลองทำการอนุมานผล (inference) กัน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e90afc-9ea6-45ad-83bb-2ac7f154b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "batched_data = converted_df[:32768]\n",
    "# Prepare the input tensor\n",
    "input_tensor = triton_grpc.InferInput(\"input__0\", batched_data.shape, 'FP32')\n",
    "input_tensor.set_data_from_numpy(batched_data)\n",
    "\n",
    "# Prepare the output\n",
    "output = triton_grpc.InferRequestedOutput(\"output__0\")\n",
    "\n",
    "# Send inference request\n",
    "response = client.infer(\"virus_prediction\", [input_tensor], outputs=[output])\n",
    "\n",
    "# Get the output data\n",
    "output_data = response.as_numpy(\"output__0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923c2923-acc1-48cb-bc41-3adafbf7eca2",
   "metadata": {},
   "source": [
    "มาดูกันว่าผลลัพธ์ที่เราได้เหมือนกับที่ใช้ **โมเดลโลคัล (local model)** หรือไม่"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f809c2db-11f9-48de-9913-3e032e14a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "xgb_data = xgb.DMatrix(input_data)\n",
    "y_test = model.predict(xgb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ada8b8-0f47-435b-90df-57c5056bfc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we got the same accuracy as previously\n",
    "#target = target.to_numpy()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "false_pos_rate, true_pos_rate, thresholds = roc_curve(target.to_numpy(), y_test)\n",
    "auc_result = auc(false_pos_rate, true_pos_rate)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(false_pos_rate, true_pos_rate, lw=3,\n",
    "        label='AUC = {:.2f}'.format(auc_result))\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "ax.set(\n",
    "    xlim=(0, 1),\n",
    "    ylim=(0, 1),\n",
    "    title=\"ROC Curve\",\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    ")\n",
    "ax.legend(loc='lower right');\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30562baa-f7c9-40d8-ab5d-40b3c642b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we got the same accuracy as previously\n",
    "#target = target.to_numpy()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "false_pos_rate, true_pos_rate, thresholds = roc_curve(target.to_numpy(), output_data)\n",
    "auc_result = auc(false_pos_rate, true_pos_rate)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(false_pos_rate, true_pos_rate, lw=3,\n",
    "        label='AUC = {:.2f}'.format(auc_result))\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "ax.set(\n",
    "    xlim=(0, 1),\n",
    "    ylim=(0, 1),\n",
    "    title=\"ROC Curve\",\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    ")\n",
    "ax.legend(loc='lower right');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00984cb-6e04-4951-a864-a67cc4a22b6f",
   "metadata": {},
   "source": [
    "อย่างที่เราเห็น **ค่า AUC** ของเราเท่ากันเลย ไม่ว่าจะใช้ตัวเลือกการอนุมาน (inference) แบบไหนก็ตาม!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a459c429-b902-4056-ad2e-05d53f82537c",
   "metadata": {},
   "source": [
    "## การวิเคราะห์ประสิทธิภาพ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e724bd09-057b-408a-896a-b48a7a4362d1",
   "metadata": {},
   "source": [
    "ก่อนหน้านี้ เราได้ทดสอบการอนุมาน (inference) ด้วยคำขอที่ **ค่อนข้าง** เล็ก แล้วถ้าเราต้องการดู **อัตราการส่งข้อมูลสูงสุด (max throughput)** ของโมเดลล่ะ? โชคดีที่ Triton มีเครื่องมือวิเคราะห์ประสิทธิภาพที่สร้างข้อมูลจำลองขึ้นมาเพื่อเก็บตัวเลขความหน่วง (latency) และอัตราการส่งข้อมูล (throughput) งั้นเรามาลองใช้กันเลย!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15129aef-3d3f-4c65-b2ff-1d0b2599ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!perf_analyzer -m virus_prediction -u \"triton:8000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fe9a4c-e5db-4474-bc9f-76e0f3485506",
   "metadata": {},
   "source": [
    "เป็นข้อมูลที่เยอะมากเลยทีเดียว ลองมาทำความเข้าใจไปทีละส่วนกัน\n",
    "\n",
    "* **Measurement window**: **ช่วงเวลา** ที่ทำการวัดผล\n",
    "* **Batch Size**: **จำนวนอินพุต** ในแต่ละคำขอ (request)\n",
    "* **Concurrency**: **จำนวนการเชื่อมต่อพร้อมกัน**\n",
    "* **Latency**: **เวลาที่ใช้ในการรับผลลัพธ์**\n",
    "* **p50/90/95/99**: **เปอร์เซ็นไทล์ต่างๆ** สำหรับค่า Latency (เช่น p50 คือเปอร์เซ็นไทล์ที่ 50 หรือค่ามัธยฐาน)\n",
    "\n",
    "จากผลลัพธ์เหล่านี้ เราจะเห็นได้ว่า **ปริมาณงาน (throughput)** ของเราอยู่ที่ประมาณ **~2300 การอนุมานต่อวินาที (inferences per second)** ด้วยการเชื่อมต่อพร้อมกันเพียงครั้งเดียว และ **ค่า Latency เฉลี่ย** สำหรับแต่ละคำขอคือ **434 ไมโครวินาที (usec)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8c5c0-7f3e-41e3-881d-38f9d0aa5d3e",
   "metadata": {},
   "source": [
    "# การปรับแต่ง Perf Analyzer\n",
    "เครื่องมือ Performance Analyzer สำหรับ Triton มีตัวเลือกมากมายที่สามารถปรับเปลี่ยนได้เพื่อวิเคราะห์ผลลัพธ์ มาเปิดใช้งานการเก็บ **เมตริก GPU** และเพิ่มค่า **ขนาดแบทช์ (batch size)** และ **ช่วงการทำงานพร้อมกัน (concurrency range)** กันเถอะ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6353c5bb-ebd5-492e-ad62-90e8929849d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!perf_analyzer --collect-metrics -m virus_prediction -u \"triton:8000\" -b 8 --concurrency-range 2:8:2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0340a9-da5a-400f-9127-b3f4fe1facad",
   "metadata": {},
   "source": [
    "ผลลัพธ์แสดงให้เห็นว่าการตั้งค่าโมเดลของเราให้ปริมาณงาน (throughput) อยู่ที่ประมาณ ~156,171 การอนุมานต่อวินาที (inferences per second)\n",
    "\n",
    "โปรดสังเกตว่ามีการเพิ่มขึ้นของปริมาณงานอย่างมีนัยสำคัญเมื่อเราเพิ่มจำนวนการเชื่อมต่อพร้อมกัน (concurrent connections)\n",
    "\n",
    "ที่ค่า **concurrency** ต่ำ Triton จะอยู่ในสถานะว่าง (idle) ในช่วงเวลาที่ส่งการตอบกลับไปยังไคลเอนต์และรับคำขอถัดไปที่เซิร์ฟเวอร์\n",
    "\n",
    "ปริมาณงานจะเพิ่มขึ้นเมื่อเราเพิ่มค่า **concurrency** เนื่องจาก Triton จะซ้อนทับการประมวลผลคำขอหนึ่งกับการสื่อสารของคำขออื่น"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea13ffda-b03d-435e-a01e-5f8ceaa076de",
   "metadata": {},
   "source": [
    "# แบบฝึกหัด\n",
    "\n",
    "โปรดใช้เวลานี้เพื่อทดลองใช้เครื่องมือ **perf analyzer** คุณสามารถดูรายการพารามิเตอร์ทั้งหมดได้ด้วยอาร์กิวเมนต์ `--help` พารามิเตอร์บางส่วนที่เราแนะนำให้ลองใช้มีดังนี้:\n",
    "\n",
    "* `-b <value>`: **ขนาดของแบตช์ (batch size)**\n",
    "* `--concurrency-range <start:end:step>`: **ช่วงของค่าความพร้อมกัน (concurrency values) ที่จะทดสอบ**\n",
    "* `--collect-metrics`: **เปิดใช้งานการเก็บรวบรวมเมตริก GPU**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb02a7-92c1-4373-afb6-c4a507ddf20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6ccd1e7-989b-49b5-8ece-0583e9890c66",
   "metadata": {},
   "source": [
    "## Model Analyzer (เครื่องมือวิเคราะห์โมเดล)\n",
    "\n",
    "แม้จะอยู่นอกขอบเขตของหลักสูตรนี้ แต่เราอยากจะแนะนำเครื่องมือ **Model Analyzer** ซึ่งเป็นส่วนหนึ่งของ Triton เครื่องมือนี้จะทำการค้นหาการตั้งค่าพารามิเตอร์ต่างๆ เพื่อค้นหาพารามิเตอร์ที่เหมาะสมที่สุดที่ช่วยเพิ่มปริมาณงานการอนุมาน (inference throughput) ให้ได้สูงสุด ด้วยการประมวลผลเล็กน้อย ผลลัพธ์สามารถดูได้ในรูปแบบ PDF ด้วยเช่นกัน ไวยากรณ์สำหรับการรัน Model Analyzer แสดงอยู่ด้านล่าง พร้อมกับตัวอย่างของผลลัพธ์ Model Analyzer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a896c-8f30-423d-baa9-db85fb66ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "## Writing constraints to file\n",
    "#cat > model_analyzer_constraints.yaml <<EOL \n",
    "#model_repository: /model_repository/\n",
    "#triton_launch_mode: \"local\"\n",
    "#latency_budget: 5\n",
    "#run_config_search_max_concurrency: 64\n",
    "#run_config_search_max_instance_count: 3\n",
    "#run_config_search_max_preferred_batch_size: 8\n",
    "#profile_models:\n",
    "#  virus_prediction\n",
    "#\n",
    "#EOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c3598a-a61f-4bcc-a079-d43e57d5d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model_analyzer profiler on XGBoost model \n",
    "#!model-analyzer profile -f model_analyzer_constraints.yaml --override-output-model-repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c0c90f-d8e6-4ce5-a9c9-197a46e1d8a2",
   "metadata": {},
   "source": [
    "![image](images/model_analyzer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb441f66-4e12-44b2-b940-2fecef8d078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8780e-671f-46e7-8754-5005fd61ae7d",
   "metadata": {},
   "source": [
    "**ทำได้ดีมาก!** ไปยัง [สมุดบันทึกถัดไป](3-08_k-means_dask.ipynb) กันเลย"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
